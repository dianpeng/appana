# specify the application's name to be summarized
name=Tiktok

# specify the description of the application, you can get it from apple store or google play store
description=TikTok is THE destination for mobile videos. On TikTok, short-form videos are exciting, spontaneous, and genuine. Whether you’re a sports fanatic, a pet enthusiast, or just looking for a laugh, there’s something for everyone on TikTok. All you have to do is watch, engage with what you like, skip what you don’t, and you’ll find an endless stream of short videos that feel personalized just for you. From your morning coffee to your afternoon errands, TikTok has the videos that are guaranteed to make your day.

# summarization options, by default I will ask LLM to give X points of pros and
# Y points of cons. User is able to specify this. If you want large # of pros
# and cons, better scale the # of reviews up
numberOfPros=5
numberOfCons=5

# specify mode of execution
# [1]
# prompt means just generate the prompt, user can use the prompt to get answer
# from other LLM
#
# [2]
# summarize, this requires configuration of llm seciton, I use local ollama
# hosted model to do summrization
run=summarize

pause[]=1
pause[]=3

# specify google play store related information for crawling the reviews
[google]
# application's app id, can be found on app's google play store page
appId=com.zhiliaoapp.musically

# # of reviews to be collected
count=5

# sorting method for reviews
sort=recent

# maximum loop, google's API does not always bounds and it can return not
# enough data back, which we need to setup a max loop count to bailout. Notes
# by default each call will scrape 50 comments
maxLoop=20
eachScrapSize=100

[apple]
# application's id, can be found on apple store page's URL
id=835599320

# # of reviews to be collected
count=5

# sorting method for reviews
sort=recent

# llm model for summarization, for now I only test it against ollama local model
# and gemma3:4b is a very smooth one which understands the instruction pretty
# well
[llm]

# gpt-oss:20b does not perform ver well on long context, ie if we have more than
# 100 reviews to be submitted.
model=gemma3:4b

# hosting server
host=http://127.0.0.1:11434

# you want to see the prompt been dumped along with response from llm.
showPrompt=false

[threshold]
# specify filter window, resolution is day, [0, 360] means between now and 360 days ago
date[]=0
date[]=180

# specify filter threshold for text length, for review that has less chars
# than specified will be ignored
text=100

# ------------------------------------------------------------------------------
# Fine grained control of score distribution with corresponding reviews

# specify an array of score to filter out. [1, 2, 4, 5]
score[]=1
score[]=2
score[]=4
score[]=5

# -----------------------------------------------------------------------------
# specify a list of weight for each score to be collected. The weight is
# distributed based on each store's propotion
# score specified:  [1,   2,     4,   5]
# weight specified: [0.4, 0.1, 0.1, 0.4]
scoreWeight[]=0.4
scoreWeight[]=0.1
scoreWeight[]=0.1
scoreWeight[]=0.4

# specify the textual language, notes the language is detected by languagedetec
# and it may have falsy situations
language[]=english
